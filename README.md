# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary
**In 1-2 sentences, explain the problem statement: e.g "This dataset contains data about... we seek to predict..."**

This dataset originated from the research of [Moro et al., 2014](https://archive.ics.uci.edu/ml/datasets/bank+marketing).
This dataset contains data about direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls. 
Often, more than one contact to the same client was required, so the data includes demographics information about the bank clients and
information about the marketing campaign and the respective phone calls. We seek to predict if the contacted client subscribed to the bank term deposit ('yes') or not ('no').

**In 1-2 sentences, explain the solution: e.g. "The best performing model was a ..."**

The best performing model was a VotingEnsemble model (a model that combines predictions from
multiple models), with an accuracy of 0.9179, trained and generated by the azure AutoML module.

## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**

The Scikit-learn pipeline used in this project comprised the following steps:
- Configuration and creation of the compute cluster (Standard_D2_V2, max_nodes = 4)
- Download and preprocessing (mainly the conversion of categorical variables, such as month, weekdays, housing, etc., into numerical values, i.e. ordinal, binary or "dummy variables") of the bank marketing dataset. These data were then split into training (80%) and test (20%) sets.
- Specification of a Logistic Regression model to predict subscriptions to the bank term deposit, a sampler for its hyperparameters (C and max_iter in this case) and a stopping policy.
- Specification and submission of the HyperDrive Experiment
- Retrieval and storage of the best model

The best Logistic Regression model trained by this pipeline (with parameters C=10, max_iter=75) yielded an accuracy of 0.9143.

**What are the benefits of the parameter sampler you chose?**

A random parameter sampler was chosen, allowing to search the hyperparameter space giving equal probability to all the parameters-values specified in the sampler. 
This makes the search less biased and less time consuming (than e.g. a fine-grained grid-search, which is more prone to be manipulated by the data-scientist 
and allocate a high computing time to combinations that yield low performance). 
The "choice" function was chosen for the sampler, limiting the random search to a set of values for the C and max_iter parameters.

**What are the benefits of the early stopping policy you chose?**

The Bandit Policy was used, allowing the experiment run to end when the primary metric is below the slack factor of the most successful run. Early termination of low performance runs allows saving training time and computing cost.

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**

The AutoML module generated 26 different models (and alerted for the class imbalance, i.e. the smallest class represents only 11% of the dataset).
The best model generated by the AutoML was a VotingEnsemble. This model comprises
the combination of predictions from several models. It was trained to
optimise classification accuracy using 5-fold cross-validation on all the training set.
This model's predictions result from a combination of weighted average predicitions from 11 models varying between
XGBoost, LightGBM, RandomForest, LogisticRegression, SGD and DecisionTrees (note that Neural Network models were blocked in this ensemble experiment).
The hyperparameters also include different data-scaling strategies for each one of these 11 models
(varying between the Standard-, TruncatedSVD, SparseNormalizer and MaxAbs- Scalers).

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**

In our case the results were quite similar between the Scikit-learn Pipeline and the AutoML (0.9143 vs. 0.9179, respectively).
This is impressive, in the sense that we needed to take several careful steps in the Scikit-learn Pipeline to train a 
sensible Logistic Regression model, and in contrast, the AutoML achieved a slightly better result with little configuration.
There is however a significant difference in the architectures of the best models, in the sense that the pipeline used a single Logistic Regression model, 
while the AutoML used a VotingEnsemble architecture, requiring predictions from 11 different models.

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**

A) In case it's not possible to get more data for the less represented class, then future experiments should 
consider balancing the number of examples per class, or re-weighting the sample's importance to 
up-weight and down-weight (i.e. balance), the less and most represented classes, respectively, to remove biases created by the class imbalance.
In case some imbalance is required then metrics that are less biased to this issue, such as the F1-score, should be considered as primary-metrics for this problem.

B) A future experiment for the Scikit-learn Pipeline should consider varying the model capacity by adding terms 
with varying degrees, or feature-combinations, and use e.g. the LASSO technique to for some initial feature selection
(as their number would increase significantly when considering high-order terms - x^2, etc, e.g. perhaps the salary of the client should have a higher weight in the model than their age - and feature-combinations).
This may also help in improving the performance of some models in the AutoML framework.
